{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "together-detail",
   "metadata": {},
   "source": [
    "# 2.3 Additional exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-radar",
   "metadata": {},
   "source": [
    "This notebook contains a wealth of additional exercises and projects for you to pick from. Since there are quite a few and you only have 2 hours during the tutorial, just choose to solve the ones you like the most.\n",
    "\n",
    "CNN can be used for a wide range of tasks that we have not all had the time to cover. These include\n",
    "\n",
    "- [Transfer Learning](#Transfer-learning): Pre-trained CNNs (e.g., on ImageNet) can be fine-tuned for specific tasks with smaller datasets. Transfer learning helps leverage features learned from large datasets. We discussed this concept in notebook 2.2.\n",
    "\n",
    "- [Object Detection](#Object-detection): CNNs can be employed for detecting and localizing objects within an image. Popular architectures include YOLO (You Only Look Once) use CNNs for object detection.\n",
    "\n",
    "- [Semantic Segmentation](#Semantic-segmentation): CNNs can be used for segmenting images into different regions or classes. Semantic segmentation is thus a type of image segmentation: The objective is to assign a semantic label (e.g., person, car, tree) to every pixel in an image. U-Net is an example of architectures used for semantic segmentation.\n",
    "\n",
    "- [Instance Segmentation](#Instance-segmentation): Similar to semantic segmentation but involves distinguishing between individual instances of objects. Instance segmentation thus extends object detection by providing a detailed pixel-level mask for each individual object instance. Mask R-CNN is a popular architecture for instance segmentation.\n",
    "\n",
    "- [Image Captioning](#Image-captioning): CNNs can be combined with Recurrent Neural Networks (RNNs) to generate captions for images. The image features are extracted using a CNN, and an RNN generates a descriptive caption.\n",
    "\n",
    "- Generative Models: CNNs are used in generative models like GANs (Generative Adversarial Networks) for generating realistic images. Conditional GANs allow controlling the characteristics of generated images.\n",
    "\n",
    "- [Image Style Transfer](#Image-style-transfer): CNNs can be used for transferring the artistic style of one image to another. Neural Style Transfer uses CNNs to achieve this.\n",
    "\n",
    "- Medical Image Analysis: CNNs are employed in tasks such as tumor detection, medical image segmentation, and disease classification. 3D CNNs are used for analyzing volumetric medical images.\n",
    "\n",
    "- Video Analysis: CNNs can be extended to video data for tasks like action recognition and video classification. 3D CNNs or 2D CNNs applied to temporal sequences are common in video analysis.\n",
    "\n",
    "- Human Pose Estimation: CNNs can be used to estimate the keypoints representing the human pose in images. Popular architectures include OpenPose and PoseNet.\n",
    "\n",
    "- [Super-Resolution](#Super-resolution): CNNs can be used for enhancing the resolution of images. SRGAN (Super-Resolution Generative Adversarial Network) is an example.\n",
    "\n",
    "- Face Recognition: CNNs are widely used for face recognition tasks. FaceNet and DeepFace are examples of architectures used for this purpose.\n",
    "\n",
    "**N.B.: Note that when you run the notebook in Google Colab, you would need to mount your Google Drive for saving (from google.colab import drive, drive.mount('/content/gdrive')) and uploading the image. Also, some of the listed examples might not work out of the box (e.g. due to differences in the versions of different libraries).** Don't worry though. If you can discuss and understand the code, we will try to solve unforseen problems like these together during the tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-genome",
   "metadata": {},
   "source": [
    "## Building CNNs\n",
    "\n",
    "**Exercise:** Create and train a neural network for image classification based on the data set FashionMNIST and the concepts you learned in notebook 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "annual-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "#[WRITE YOUR OWN CODE BELOW]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-skirt",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "\n",
    "**Exercise**: Retrain a neural network following the steps in notebook 2.2. Choose a suitable neural network architecture and dataset yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-salmon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "shared-problem",
   "metadata": {},
   "source": [
    "## Object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-caribbean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-21 16:26:21--  https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n",
      "Resolving www.cis.upenn.edu (www.cis.upenn.edu)... 158.130.69.163\n",
      "Connecting to www.cis.upenn.edu (www.cis.upenn.edu)|158.130.69.163|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 53723336 (51M) [application/zip]\n",
      "Saving to: ‘data/PennFudanPed.zip.1’\n",
      "\n",
      "PennFudanPed.zip.1  100%[===================>]  51.23M  12.0MB/s    in 4.7s    \n",
      "\n",
      "2024-02-21 16:26:26 (11.0 MB/s) - ‘data/PennFudanPed.zip.1’ saved [53723336/53723336]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the zip file\n",
    "!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip -P data\n",
    "\n",
    "# Unzip the file in the data directory (suppressing output)\n",
    "!unzip data/PennFudanPed.zip -d data >/dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# Function to read an image and convert it to a PyTorch tensor without normalization\n",
    "def read_image(image_path):\n",
    "    return F.to_tensor(Image.open(image_path).convert(\"RGB\"))\n",
    "\n",
    "# Load pre-trained Faster R-CNN model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True, progress=True)\n",
    "model.eval()\n",
    "\n",
    "# Example: Load an image\n",
    "image_path = \"data/PennFudanPed/PNGImages/FudanPed00046.png\"\n",
    "image = read_image(image_path)\n",
    "\n",
    "# Make predictions on the image\n",
    "with torch.no_grad():\n",
    "    prediction = model([image])  # Send the image through the model\n",
    "    boxes = prediction[0]['boxes'].tolist()  # Convert boxes to a list\n",
    "    labels = prediction[0]['labels'].tolist()  # Convert labels to a list\n",
    "\n",
    "# Convert to NumPy array for displaying with matplotlib\n",
    "image_np = F.to_pil_image(image.mul(255).byte())\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "# Draw bounding boxes manually using matplotlib\n",
    "draw = ImageDraw.Draw(image_np)\n",
    "for box, label in zip(boxes, labels):\n",
    "    draw.rectangle(box, outline='red', width=2)\n",
    "\n",
    "# Display the image with bounding boxes and labels\n",
    "plt.imshow(image_np)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-scotland",
   "metadata": {},
   "source": [
    "That's quite a lot of boxes... what did the network detect? The Faster R-CNN model in torchvision for ResNet50 backbone is trained on the COCO dataset, so we can translate the labels into human-readable chategories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COCO class names\n",
    "coco_classes = [\n",
    "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck',\n",
    "    'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench',\n",
    "    'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe',\n",
    "    'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n",
    "    'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet',\n",
    "    'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven',\n",
    "    'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "    'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "# Map numeric labels to COCO class names\n",
    "class_names = [coco_classes[label - 1] for label in labels]  # Subtract 1 as COCO labels start from 1\n",
    "\n",
    "# Print the list of class names\n",
    "print(\"Detected Class Names:\", class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-bryan",
   "metadata": {},
   "source": [
    "Okay, so it found a lot of bicycles and people which is certainly correct. But also skis... At any rate, you are probably not interested in the COCO classes for scientific purposes and want to retrain the network to provide output based on another set of classes. \n",
    "\n",
    "**Exercise**: Have a look at [PyTorch's homepage](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) to learn how to retrain a CNN for object detection. Warning: While you can run the code in Google Colab out of the box (and the Google Colab script is provided), training the networks will take hours. So, unless you are able to run the code on your own computer, limit yourself to discussing the content of the script.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-screw",
   "metadata": {},
   "source": [
    "## Semantic segmentation\n",
    "\n",
    "**Exercise**: You can find a rudimentary example of semantic segmentation on [PyTorch's homepage](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/). Here, you use a pretrained network from Deeplabv3. Run and discuss the code.\n",
    "\n",
    "**Exercise**: When looking for external resources, Kaggle is a useful source. Have a look at one of the examples on their [homepage](https://www.kaggle.com/code/ligtfeather/semantic-segmentation-is-easy-with-pytorch). Note that this examples includes a training phase. Discuss the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-venice",
   "metadata": {},
   "source": [
    "## Instance segmentation\n",
    "\n",
    "**Exercise**: You can find an example of semantic segemenation with object detection, i.e. instance segmentation, on [PyTorch's homepage](https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_e2e.html). Discuss the code in your group. Find more resources on the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-basin",
   "metadata": {},
   "source": [
    "## Image captioning\n",
    "\n",
    "**Exercise**: You can find an example on how to do image captioning with CNN on [Kaggle](https://www.kaggle.com/code/mdteach/image-captioning-with-attention-pytorch). Discuss the code example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-ethiopia",
   "metadata": {},
   "source": [
    "## Image style transfer\n",
    "\n",
    "**Exercise**: Explore the topic through the examples on [PyTorch's homepage](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html) and [Kaggle](https://www.kaggle.com/code/parthplc/pytorch-project-1-neural-style-transfer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-report",
   "metadata": {},
   "source": [
    "## Super-resolution\n",
    "\n",
    "**Exercise**: Explore the topic on [PyTorch's homepage](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html?highlight=onnx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-particular",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
