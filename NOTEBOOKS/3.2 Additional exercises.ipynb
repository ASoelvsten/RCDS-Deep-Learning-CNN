{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "premium-hydrogen",
   "metadata": {},
   "source": [
    "# 3.2 Additional exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-sustainability",
   "metadata": {},
   "source": [
    "This notebook contains a wealth of additional exercises and projects for you to pick from as well as some glosary. Since there are quite a few and you only have 2 hours during the tutorial, just choose to solve the ones you like the most.\n",
    "\n",
    "- [More on cross validation](#More-on-cross-validation)\n",
    "- [Regularisation techniques](#Regularisation-techniques)\n",
    "- [Momentum](#Momentum)\n",
    "- [Learning rate scheduling](#Learning-rate-scheduling)\n",
    "- [Batch normalisation](#Batch-normalisation)\n",
    "- [Weight initialization](#Weight-initialization)\n",
    "- [Gradient clipping](#Gradient-clipping)\n",
    "- [Warm-up steps](#Warm-Up-steps)\n",
    "- [Ensemble methods](#Ensemble-methods)\n",
    "- [Monitor and visualise](#Monitor-and-visualise)\n",
    "\n",
    "## More on cross validation\n",
    "\n",
    "**Exercise 1**: As mentioned in notebook 3.1, you don't really need to write the code for cross validation yourself. Suitable methods have already been implemented, e.g., in scikit-learn. **a)** However, to make PyTorch work with scikit-learn, you would need wrap it in [skorch](https://skorch.readthedocs.io/en/stable/index.html). Explore skorch. **b)** Explore [RayTune](https://docs.ray.io/en/latest/tune/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-organic",
   "metadata": {},
   "source": [
    "## Regularization techniques\n",
    "\n",
    "You can improve your network through regularisation techniques, such as dropout or L1/L2 regularization to prevent overfitting and enhance model generalization. You met dropout in the exercises of notebook 2.1. L1/L2 regularisation simply means that you add a penealy term to your current loss, discouraging large parameters (for a neural network, this means that you try to keep the weights small).\n",
    "\n",
    "**Exercise 2**: You can include L2 regularisation (what is that exactly?) by setting weight\\_decay to a non-zero value in your optimiser. What exactly would you need to do in your code? What does weight\\_decay represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-pathology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-finnish",
   "metadata": {},
   "source": [
    "**Exercise 3**: You could also add your L2 regularisation manually. To see how you might do this, have a look at the following example from [Kaggle](https://www.kaggle.com/code/cheesleypringlesman/minimizing-loss-using-l1-regularization-in-pytorch) on L1 regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-purse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "arranged-rescue",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "\n",
    "To improve convergence when using stochastic gradient decent, we can draw on the concept of momentum from physics. Thus, in momentum-based SGD, the update is influenced not only by the current gradient but also by an exponentially decaying moving average of past gradients. This way, if the optimizer has been consistently moving in a certain direction over the last few steps, it will continue to do so, building up momentum. But how much should the previous previous gradients contribute? For this purpose, you can set a hyperparameter.\n",
    "\n",
    "**Exercise 4**: How do you do this in practice in PyTorch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-quest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "interracial-franchise",
   "metadata": {},
   "source": [
    "**Exercise 5**: What does the term \"Exponential Moving Average\" cover?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-mauritius",
   "metadata": {},
   "source": [
    "## Learning rate scheduling\n",
    "\n",
    "**Exercise 6**: [Learning rate scheduling](https://pytorch.org/docs/stable/optim.html) is a technique used during the training of neural networks where the learning rate is adjusted over time according to a predefined schedule. The goal is to improve the training process, potentially speeding up convergence, enhancing model performance, and achieving better generalization. Construct a simple coding example implementing learning rate scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-index",
   "metadata": {},
   "source": [
    "## Batch normalisation\n",
    "\n",
    "Before we pass the data to the neural network, we normalise it. However, as the input data $x$ gets transformed, passing through each layer, $x$ might very well blow up significantly. To avoid this, we can normalise the output each layer, using [nn.BatchNorm2d()](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) after convolutional or pooling layers and [nn.BatchNorm1d()](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) after fully connected layers (why?). This approach is called batch normalisation (see also the orginal article by [Ioffe and Szegedy](https://arxiv.org/abs/1502.03167)).\n",
    "\n",
    "**Exercise 7**: Explain the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "gothic-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple CNN with Batch Normalization\n",
    "class CNNWithBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNWithBatchNorm, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.batchnorm_fc = nn.BatchNorm1d(128)\n",
    "        self.relu_fc = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(-1, 64 * 7 * 7) # Alternative to flatten\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchnorm_fc(x)\n",
    "        x = self.relu_fc(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-rolling",
   "metadata": {},
   "source": [
    "## Weight initialization\n",
    "\n",
    "Weight initialisation is a crucial aspect of training neural networks. It involves setting the initial values of the weights in the network before training begins. Proper weight initialisation can help improve the convergence speed and the overall performance of the neural network. PyTorch does this automatically, and weight initialisation may not be explicitly included in basic examples because the default initialisation methods provided by modern deep learning frameworks are generally well-suited for many common scenarios. Frameworks like PyTorch thus use sensible default initialisation strategies, such as Xavier/Glorot initialisation for linear layers. \n",
    "\n",
    "**Exercise 8**: But you can set decide on the initialisation yourself. Check out nn.init.xavier_uniform_(). What does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "continental-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=10, out_features=5)\n",
    "        # Explicitly set Xavier/Glorot initialization for the linear layer\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-cathedral",
   "metadata": {},
   "source": [
    "## Gradient clipping:\n",
    "\n",
    "Gradient clipping helps to prevent exploding gradients during the optimization process. You can find the corresponding tools in [PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html). Gradient clipping is most commonly used for recurrent neural networks (RNNs) and other models that involve sequential data processing, where the vanishing or exploding gradient problem is prevalent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-analyst",
   "metadata": {},
   "source": [
    "## Warm-up steps\n",
    "\n",
    "You can gradually increase the learning rate during the initial steps of training. This approach can help the model converge more quickly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-wednesday",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "Ensemble methods involve training multiple models and combine their predictions. The idea is that diverse models can collectively produce more accurate and robust predictions. Indeed, many machine learning models draw on ensemble methods (cf. random forest, boosting and bagging). Also, in Deep Learning, you can find various ensemble methods. These include but are not limited to\n",
    "\n",
    "- Model Averaging: Train multiple instances of the same deep learning architecture with different random initializations or hyperparameters and average the predictions of these models during inference.\n",
    "- Bagging with neural networks: Train multiple instances of the same neural network on different subsets of the training data and average predictions during inference.\n",
    "- Weight averaging: Instead of combining predictions at the decision level (as in voting or stacking), weight averaging involves combining the weights of multiple trained models to create a single model with averaged weights. \n",
    "\n",
    "**Exercise 9**: Investigate this topic further. What does PyTorch offer in this regard (discuss briefly e.g. code found [here](https://pytorch.org/docs/stable/optim.html) and [here](https://pytorch.org/tutorials/intermediate/ensembling.html#:~:text=Model%20ensembling%20combines%20the%20predictions,vmap%20.))?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-handle",
   "metadata": {},
   "source": [
    "# Monitor and visualise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-success",
   "metadata": {},
   "source": [
    "TensorBoard can be used with PyTorch to visualise and analyse the training of neural networks. It offers real-time visualisation of training metrics, and it includes an interactive interface. Moreover, you can easily compare multiple training runs or experiments in TensorBoard.\n",
    "\n",
    "**Exercise 10**: Explore Tensorboard for PyTorch (e.g. [here](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) and [here](\n",
    "https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/tensorboard_with_pytorch.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
